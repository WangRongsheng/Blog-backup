---
title: 《深度学习入门》阅读笔记
date: 2019-09-07 10:20:41
thumbnail: https://i.loli.net/2019/09/29/8oIDBKq9sW4aRwG.png
tags: 深度学习
categories: 阅读笔记
---
# 第1章 Python入门

## 1.5.4  Numpy的N维数组

```python
import numpy as np
a = np.array([[1,2],[3,4]])
b = np.array([[3,0],[0,6]])
print(a+b)
print(a*b)
```

<!--more-->

**注意**:数学上，一维数组称为向量；二维数组称为矩阵；可以将一般化后的向量或矩阵等统称为张量。

## 1.5.5 广播

```python
import numpy as np
a = np.array([[1,2],[3,4]])
b = np.array([10,20])
print(a*b)
```

## 1.6.3 显示图像

```python
import matplotlib.pyplot as plt
from matplotlib.image import imread
img = imread('图片名称')
plt.imshow(img)
plt.show()
```

# 第2章 感知机

感知机是神经网络(深度学习)的起源算法。

## 2.3.3 使用权重和偏置的实现

权重值是控制输入信号的重要参数；偏置值调整了神经元被激活的容易程度。

## 2.4.2 线性和非线性

单层感知机的局限性在于它只能表示由一条直线分割的区间。

## 2.5.2 异或门的实现

叠加了多层的感知机称为多层感知机。

感知机的层数叫法问题。

## 2.6 从与非门到计算机

实际上，使用感知机甚至可以表示计算机！

# 第3章 神经网络

当拥有感知机的同时我们也知道了两个消息：

1. 好消息：对于复杂的函数，感知机也能通过叠加层数来有可能性的实现。

2. 坏消息是：**设定权重的工作在感知机中仍只能是由人工进行的**。

而神经网络的出现就是为了解决来自感知机的坏消息。

### 3.1.3 激活函数登场

激活函数的作用在于决定如何来激活输入信号的总和。

![微信截图_20190907111807.png](https://i.loli.net/2019/09/07/UgvDeCzXq1MoBdV.png)

### 3.2 激活函数

阶跃函数：函数以阈值为界，一旦输入超过阈值，就切换输出。

实际上，**如果将激活函数从阶跃函数换成其他函数，我们就可以进入到神经网络的世界了**。

### 3.2.5 sigmiod函数和阶跃函数的比较

sigmoid函数是一条平滑的曲线，输出随着输入发生连续性变化；而阶跃函数以0为界，输出发生急剧性的变化。
sigmoid函数的平滑性对神经网络的学习具有重要的意义。

也就是说，相对于感知机中的神经元只能返回0或1的信号，神经网络中返回的是连续的实数值信号。

#### 相同点

- 两者的图像结构均表示为：“输入小时，输出接近0（为0）；输入大时，随着输入的增大，输出靠近1（为1）”。

- 不管输入的大小为多少，输出信号的值始终在0到1之间。

- 均为非线性函数。

#### 不同点

- 阶跃函数：“竹筒敲石”。

- sigmoid函数：“水车”。

### 3.2.6 非线性函数

输出值为输入值的常数倍的函数称为线性函数。

为了发挥叠加层的优势，神经网络必须使用非线性函数。

### 3.5 输出层的设计

神经网络可以使用在分类和回归问题上，不过需要根据情况改变**输出层的激活函数** ，一般而言，回归问题用恒等函数，分类问题用softmax函数。

softmax函数python实现：
```python
def softmax(a):
	exp_a = np.exp(a)
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a
	return y
```

### 3.5.2 使用softmax函数时的注意事项

softmax函数的分子进行了指数的运算，可能会产生一些超大值，如果这些超大值进行除法运算，会出现"不确定"的情况，这就是产生了溢出问题。

改进的softmax函数python实现：
```python
def softmax(a):
	c = np.max(a)
	exp_a = np.exp(a - c)
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a
	return y
```

### 3.5.4 输出层的神经元数量

输出层的神经元数量需要根据需要解决的问题来决定。对于分类问题，输出层的神经元的数量一般设为类别的数量。

## 第4章 神经网络的学习

“学习”是指从训练数据中自动获取**最优权重参数**的过程。

### 4.1.1 数据驱动

对于一个数字“5”的识别，我们可以采用一些方法来识别：

- 人暴力想出一个算法识别，得出答案。-人参与

- 人想到特征量（如一个横，一个类似s构成了5），然后采用机器学习（SVM，KNN）得出答案。-人参与

- 神经网络（深度学习）利用数据学习，机器自己识别判断。-完全机器

深度学习也被称为端到端的机器学习。

神经网络的优点是对所有问题都可以采用同样的流程来解决，不管解决的是识别数字还是人脸，神经网络都是通过不断的学习所提供的数据，尝试发现待解决的问题。

### 4.1.2 训练数据和测试数据

- 训练数据：也称**监督数据**，用来训练新的模型的数据。

- 测试数据：为了检验模型的泛化能力。

泛化能力指处理未被观察过的数据（不包含在训练数据中的数据）的能力。

获得泛化能力是机器学习的最终目标。

### 4.4.1 梯度法

根据寻找最小值还是最大值，寻找最小值的梯度法称为梯度下降法，寻找最大值的梯度法称为梯度上升法。但是通过反转损失函数的符号，求最大和最小值会变成相同的问题，所以一般来说，神经网络（深度学习）中，梯度法指的是梯度下降法。

## 第5章 误差反向传播法

### 5.4 简单层的实现

Affine层是负责矩阵乘积的。

## 第6章 与学习相关的技巧

### 6.1.3 SGD的缺点

为了改正SGD的缺点，我们可以使用优化算法Momentum，AdaGrad，Adam等。

## 第7章 卷积神经网络

## 第8章 深度学习
























