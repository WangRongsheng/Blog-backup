# 简介

人工智能技术可以让机器帮助人类**做出做好的决策** 。这个决策是在某些限定条件下的取舍。

例如，小明在填写高考志愿时，需要在多所目标高校中选择最合适自己的那所。假设在这个过程中，小明的决策依据是学校综合实力和被录取可能性的求和。对于北大、北理工、北大青鸟，三所学校的综合实力排名是递减的关系；而对于小明而言，他被录取的可能性是递增的关系。因此，在决策志愿时，就需要综合考虑这两个因素，以保证自己的考学利益最大化。此时，这个问题就是一个最优化决策的数学问题。

<center><img src="https://s1.ax1x.com/2020/04/10/GIW3dI.png" alt="1" border="0"></center>
<center><img src="https://s1.ax1x.com/2020/04/10/GIW1eA.png" alt="2" border="0"></center>

这个例子非常简单，可能简单扫一眼，人们就能得到最优决策的结果。能够快速决策的一个重要原因是它的决策变量只有选择某个学校这一个。而对于更加复杂的最优化决策问题，假设其决策变量有成千上万个，而决策结果受这成千上万个变量的共同影响时，人们作出最优决策将会变得非常困难。此时就是人工智能发挥作用的重要场景。

不管是简单还是复杂的最优化决策，其本质都是个数学问题，它与求解一个函数的极大值或极小值非常相似。机器学习的很多初学者可能会对数学具有恐惧心理，但我想告诉大家，当你掌握数学背后的逻辑和套路后，数学就是纸老虎。

# 极值

> 假设有这样一个目标函数 y = f(x)，求解极值就是找到在某个定义域下求 y 的最大值或最小值。

## 例1

目标函数 y = sinx 在定义域 [0,2π] 的区间内，当 x 取值 π/2 时，y 可以达到最大值 1。

对于这样的极值求解问题，相信你一定并不陌生。原因在于高中数学的函数知识就是围绕着这些问题做文章的。高中数学告诉我们，极值的特点是**导数为0** 。因此，最常规的解法就是<u>求解目标函数的一阶导数，令导数等于0，并求解这个方程</u>。

## 例2

y=sinx的一阶导数y'=cosx。令导数为0，则cosx=0。在[0,2π]的定义域区间内，我们发现这个方程在x=π/2和3π/2时为 0。这分别对应着极大值和极小值。然后，分别把这两个极值点代入目标函数，你会发现，当 x = π/2 时，目标函数取得极大值 +1；当 x = 3π/2 时，目标函数取得极小值 -1。

## 例3

求解x1和x2分别为多少时？目标函数y=(x1-1)2+(x2-3)2可以取得最小值。

> [第二节 偏导数](http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530802.htm)

此时，我们需要分别计算y关于x1和x2的偏导，并令这两个导函数为 0，求解这个方程组。根据计算你会得到两个方程，分别是 2(x1 - 1) = 0 和 2(x2 - 3) = 0。最终解得，当 x1 = 1、x2 = 3 时，函数取得最小值。

高中解法简单粗暴，适用于绝大多数的极值求解问题。然而，现实中常常会遇到方程求解复杂的情况，这就是高中数学解法的**局限性** 。

## 例4

求解x为多少时，目标函数是y=sinx+5x2+2可取得最小值。

那么，你在求解一阶导数后得到y'=cosx+10x并令其为0。到这里都没有问题，只是最后的方程求解过程可能会让你不知所措。面对这种情况，我推荐使用**梯度下降法** 来解决问题。

# 梯度下降

我们看一下函数的梯度的概念。关于函数的梯度，需要你记住以下 4 点。

1. 梯度的计算方式是求解函数关于每个变量的一阶偏导；

2. 梯度的标记为反三角 ▽；

3. 梯度是个向量；

4. 梯度表示函数变化率最大的方向。

## 例1

计算函数 y = (x1 - 1)2 + (x2 - 3)2 的梯度。

根据定义，求解函数关于每个变量的一阶偏导 

<center><img src="https://s1.ax1x.com/2020/04/12/Gq5OKI.png" alt="11" border="0"></center>


分别计算 y 关于 x1 和 x2 的偏导，则有

<center><img src="https://s1.ax1x.com/2020/04/12/Gq5qxA.png" alt="12" border="0"></center>


我们发现梯度向量是个关于原函数自变量 x1 和 x2 的函数，毕竟函数的导数也是个函数。特别需要注意，我们对于某组自变量 x1 = 1、x2 = 2，则在这个点的梯度为       

<center><img src="https://s1.ax1x.com/2020/04/12/Gq5b2d.png" alt="13" border="0"></center>

这就表示在对于点 (1,2)，方向 [0,-2] 是变化率最快的。

<center><img src="https://s1.ax1x.com/2020/04/12/Gq5Xrt.png" alt="14" border="0"></center>

梯度的计算方式虽然很简单，但它对于求解极值的作用却非常大。我们以爬山到山顶为例。为了最快到达山顶，你一定会选择走海拔变化最快的路径。最终不断前进到达了山顶。这个山顶就是海拔的最大值。梯度下降法就是借鉴这个思想来求解极值的，它也是最优化问题中最常用的优化算法。

梯度的计算流程是这样的：
1. 梯度下降法会随机初始化一组自变量。
2. 计算目标函数在这个自变量上的梯度，并用梯度方向去更新自变量。通过不断循环执行第二步，最终得到极值。

这个流程形象来说，就是在山上随机初始化一个点，然后找到这个点的上山最快的方向并沿着这个方向前走一小步。通过重复多轮尝试找到最快的方向并走一小步，最终可以抵达山顶。通常更新自变量的过程会设置一个系数α，叫作学习率。那么更新的方程式就是

<center><img src="https://s1.ax1x.com/2020/04/12/GqIQz9.png" alt="GqIQz9.png" border="0" /></center>

> 值得说明的是，如果求解的是极大值，那么学习率 α 大于 0；如果求解的是极小值，那么 α 为负。

## 例2

假设目标函数是y=sinx+5x2+2，求解这个函数的最小值。利用高中方法，你可能会卡在最后的方程求解上。现在我们利用梯度下降法来计算。因为求解极小值，所以设置学习率为-0.1。

首先随机初始化 x，假设为 0。此时目标函数为单变量函数，所以梯度是个单变量的向量。根据梯度定义，计算梯度为 cosx + 10x。

- 第1轮循环，0点的梯度值为1。根据公式更新结果为0-0.1×1=-0.1。
- 第2轮循环，计算-0.1处的梯度为-0.0050，更新结果为-0.1－0.1×(-0.0050)=-0.0995。
- 第3轮循环，计算-0.0995处的梯度已经约等于0了。因此，我们得到结果，在x=-0.0995时，函数一阶偏导为 0.00005 约等于 0，目标函数值取得极小值，为 1.95。

## 例3

利用梯度下降法，计算 y = (x1 - 1)2 + (x2 - 3)2 的极小值。

此时为多变量的目标函数，因此梯度是个向量，为       

<center><img src="https://s1.ax1x.com/2020/04/12/GqI2FS.png" alt="GqI2FS.png" border="0" /></center>

由于求解极小值，故设置学习率为负数 -0.4。

首先，随机初始化自变量，假设为 [0,0]。接着进入梯度下降的循环。

- 第1轮循环，计算[0,0]处的梯度值，为[-2,-6]；更新参数的结果为[0,0]-0.4×[-2,-6]=[0.8,2.4]。
- 第2轮循环，计算[0.8,2.4]处的梯度值，为[-0.4,-1.2]；更新参数的结果为[0.96,2.88]。
- 第3轮循环，计算[0.96,2.88]处的梯度值，为[-0.08,-0.24]；更新参数的结果为 [0.99,2.98]。
- 第4轮循环，计算 [0.96,2.88] 处的梯度值，为 [-0.02,-0.05]；更新参数的结果为 [1.0,3.0]。

后续继续循环，结果不再发生改变，得到目标函数的最小值。

梯度下降法是求解极值问题中，最常用、最经典的算法。也是机器学习中，逻辑回归、神经网络中常用的优化方法。







