# 简介

我们主要学习与机器学习相关的线性代数知识，主要包括向量和矩阵的乘法、范数、求导等基本运算，及其在机器学习中的应用等内容。

<center><img src="https://s1.ax1x.com/2020/04/13/GX8hgs.png" alt="GX8hgs.png" border="0" /></center>

线性代数是数学的一个分支。相信你在大学时，一定学习过这门课程，甚至可能会为通过考试而熬夜苦战。根据我的感受，线性代数这门课并不简单，但是比高等数学还是要容易一些。从机器学习的视角来看，线性代数是必须要了解的，但不需要达到精通的程度。为了不让线性代数成为学习机器学习的绊脚石，你需要掌握向量、矩阵的各种基础运算。值得一提的是，当你掌握线性代数的逻辑和套路时，线性代数就是纸老虎。

<center><img src="https://s1.ax1x.com/2020/04/13/GX8TbV.png" alt="GX8TbV.png" border="0" /></center>

既然名字叫作线性代数，那么它一定是线性模型的前置基础知识。线性代数最基本的研究对象是向量，向量的向量又组成了矩阵。有了向量或者矩阵，就能将很多数字用一个向量符号表示，甚至可以将很多高维的数据用一个矩阵来表示。因此，你可以理解为线性代数是处理大数据的基础。

# 向量的基本运算

<center><img src="https://s1.ax1x.com/2020/04/13/GX8zK1.png" alt="GX8zK1.png" border="0" /></center>

我们首先来学习向量的知识。在这里只需学习与机器学习有关的内容。我们在高中时就开始接触向量，它的基本运算并不会困扰我们。向量是一个有方向的量，它的表示形式是斜体加粗的小写字母或者斜体小写字母上加一个向右的箭头。别忘了，在上一课时计算函数的梯度时，梯度也是一个向量，表示的也是一个有方向的量，是函数值变化率最快的方向。

## 点乘

<center><img src="https://s1.ax1x.com/2020/04/13/GXGIRH.png" alt="GXGIRH.png" border="0" /></center>

除了普通的加法以外，向量的另一个重要运算就是点乘了。点乘两个相同维数的向量，可以得到一个常数。点乘的计算方法是两个向量对应项乘积之和。例如，计算向量[1,1]和[-1,2]的点乘，计算过程就是 1*(-1)+1*2，结果等于 1。

# 矩阵的基本运算

<center><img src="https://s1.ax1x.com/2020/04/13/GXJa6A.png" alt="GXJa6A.png" border="0" /></center>

接着我们来看一下矩阵。你应该是上了大学之后才开始接触矩阵的。矩阵可以形象的理解为是向量的向量，通常用加粗大写字母表示。在机器学习中，通常会用一个矩阵来表示一个大数据集。其中每一行是一个样本的不同维度，列方向则是集合中的每一条数据样本。例如如图所示的 3 名同学 3 门课的考试成绩。

用矩阵来表示每人每门课的成绩，则是

<center><img src="https://s1.ax1x.com/2020/04/13/GXJB0P.png" alt="GXJB0P.png" border="0" /></center>

特别需要注意的是，当数据集中只有一条数据时，这个矩阵就退化为一个向量。因此，你也可以理解为，向量也是一个特殊的矩阵。

## 转置

矩阵有一个很重要的运算，就是转置。在矩阵右上角，用大写字母 T 表示。它能让矩阵的行列互换，原本 m x n 的矩阵就变成了 n x m 的新矩阵。

例如原本是

<center><img src="https://s1.ax1x.com/2020/04/13/GXJR6s.png" alt="77" border="0"></center>

转置后变成了

<center><img src="https://s1.ax1x.com/2020/04/13/GXJ2lj.png" alt="78" border="0"></center>

<center><img src="https://s1.ax1x.com/2020/04/13/GXJokT.png" alt="GXJokT.png" border="0" /></center>

转置在机器学习中有着非常重要的数学意义。你在学习机器学习时，也很可能会被各种各样的转置符号给弄晕。这是因为在机器学习中，关于向量和矩阵有着默认的规则。在机器学习中，默认所有的向量为列向量。当你必须要表示行向量时，则需要将向量转置。以前面 3 个同学的成绩为例。每个人 3 门课的成绩是个向量。根据列向量的准则，则有

<center><img src="https://s1.ax1x.com/2020/04/13/GXJx76.png" alt="80" border="0"></center>

总成绩是个矩阵，它是向量的向量，因此也需要满足列向量的准则。因此有：

<center><img src="https://s1.ax1x.com/2020/04/13/GXJv0x.png" alt="81" border="0"></center>

此时就需要借助转置符号，变列为行来表示其中每个人的成绩。在这里，我们再次重申一遍，在整个机器学习中，默认所有的向量为列向量。

## 乘法

矩阵的乘法，在机器学习中被高频使用。我们举个例子来说明如何计算矩阵的乘法。如果有

<center><img src="https://s1.ax1x.com/2020/04/13/GXYM9g.png" alt="82" border="0"></center>

则：

<center><img src="https://s1.ax1x.com/2020/04/13/GXYQ3Q.png" alt="83" border="0"></center>

这里需要注意，矩阵的乘法对维数有严格要求。第一个矩阵的列数必须与第二个的行数相等。维数不匹配的矩阵不可以相乘。

## 哈达玛积

除了乘法以外，矩阵的基本运算还有哈达玛积。它要求两个矩阵的维数完全相同，计算方式是对应项元素的乘积。例如，

<center><img src="https://s1.ax1x.com/2020/04/13/GXYdCF.png" alt="84" border="0"></center>

则其哈达玛积的结果为

<center><img src="https://s1.ax1x.com/2020/04/13/GXYU4U.png" alt="85" border="0"></center>

## 求逆


关于矩阵，你还需要掌握求逆运算。求逆运算只可以作用在行数等于列数的方阵上，用右上角标 -1 来表示，可以得到一个矩阵的逆矩阵。逆矩阵满足这样的性质，它和原矩阵相乘后，可以得到单位矩阵，即主对角线元素为 1，其他元素为 0 的方阵。例如，

<center><img src="https://s1.ax1x.com/2020/04/13/GXYyHx.png" alt="GXYyHx.png" border="0" /></center>

关于求逆，你不需要会各种手动计算的方法，能用 Python 的计算包或者 Matlab 求解就可以了。

# 线性代数与机器学习

## 范数

到现在为止的知识点，相信都还难不倒你。那么接下来的内容会渐渐开始有一些挑战。我们先看一下范数。范数可以对矩阵和向量去计算，它是泛函分析中的重要知识。但是从机器学习的视角来看，我们不需要掌握那么复杂的内容。在这里，我们只需要学习向量的 L1 范数和 L2 范数；其他更复杂的内容，如果你感兴趣，可以翻阅相关的数学书籍。

向量的 L1 范数计算方式是各个元素的绝对值之和。向量的 L2 范数计算方式是各个元素平方之和的平方根。例如，

<center><img src="https://s1.ax1x.com/2020/04/13/GXYHVP.png" alt="87" border="0"></center>

则

<center><img src="https://s1.ax1x.com/2020/04/13/GXYTbt.png" alt="88" border="0"></center>

也可以表示为

 <center><img src="https://s1.ax1x.com/2020/04/13/GXYoDI.png" alt="89" border="0"></center>

## 求导

对于矩阵、向量的求导，可能是本课时唯一的难点。这里的知识点，需要你在理解的基础上独立完成求导过程。在机器学习中，对于矩阵的求导用的会比较少，这个知识点并不是必须掌握的。而对于向量a关于向量b的求导，则必须掌握。原因在于机器学习的未知变量通常是模型的系数或系数组，而学习的标签是真实值向量。这个系数组和标签真实值，通常以向量的形式存在，例如线性回归、逻辑回归等模型。其他更复杂的求导，如果你感兴趣可以自己查阅相关资料。

接下来，看一下向量关于向量的求导。向量 y 关于向量 w 的求导结果是向量 y 中每个元素关于向量 w 中每个元素求导结果的矩阵。如果向量 w 的维数为 n x 1，向量 y 的维数是 m x 1，则求导之后的矩阵维数就是 n x m。特别需要注意，当 m=1 时，向量 y 是个常数，此时定义同样成立。

<center><img src="https://s1.ax1x.com/2020/04/13/GXt6Mj.png" alt="GXt6Mj.png" border="0" /></center>

掌握了求导的定义之后，就可以利用它去求解导数啦。我们在这里把后续机器学习建模会用到的内容进行分析。首先看矩阵和向量的乘积结果。

<img src="https://s1.ax1x.com/2020/04/13/GXt7QJ.png" alt="GXt7QJ.png" border="0" />

这两个求导结果可以通过简单的推导得到。在此需要你记住求导过程。

# 总结

<center><img src="https://s1.ax1x.com/2020/04/13/GXNulQ.png" alt="GXNulQ.png" border="0" /></center>

本课时，我们对线性代数的复习只是线性代数知识的冰山一角。但是这些知识，对我们突破机器学习已经足够了。我们先复习了矩阵和向量的基本运算，这些是为了讲述范数和求导作准备。当我们掌握了范数和求导法则之后，就具备了突破机器学习中线性代数的能力。

其中，范数的知识，是在机器学习中克服模型过拟合的重要手段。不管是L1还是L2，范数可被用作损失函数的惩罚项，也称作正则项，用来指导模型的学习训练。范数值越大，说明模型参数绝对值整体偏高，则说明模型的复杂度偏高。自然地，过拟合风险也就比较高。关于过拟合的知识，我会在后面专门拿出一个课时来讲解。另外，线性代数作用在线性模型居多。线性回归是回归的入门级算法。
对于线性回归模型的最优化过程，需要大量的向量求导计算。

如果不具备这些基础知识，你可能就会被一个非常简单的入门级算法困扰，这是非常划不来的事情。不过现在好了，这些必备的知识点你都已经突破了。其余线性代数的知识，如果你感兴趣，可以花时间去进行专门、系统地学习。如果你对数学比较抵触，掌握这些内容，已经足够你驰骋机器学习啦。










